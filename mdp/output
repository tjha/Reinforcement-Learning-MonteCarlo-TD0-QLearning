uniformly random policy evaluation:
[[ 2.64  8.54  3.97  5.09  0.94]
 [ 0.98  2.6   1.87  1.57  0.11]
 [-0.36  0.43  0.4   0.09 -0.75]
 [-1.31 -0.67 -0.56 -0.8  -1.48]
 [-2.23 -1.62 -1.48 -1.68 -2.32]]
optimal value function after policy iteration
[[18.88 21.5  18.88 16.5  14.49]
 [16.58 18.88 16.58 14.56 12.78]
 [14.56 16.58 14.56 12.78 11.22]
 [12.78 14.56 12.78 11.22  9.85]
 [11.22 12.78 11.22  9.85  8.65]]
optimal policy after policy iteration
  e    nesw   w    nesw   w   

  e     n     nw    w     w   

  e     n     nw    nw    nw  

  e     n     nw    nw    nw  

  e     n     nw    nw    nw  

optimal value function after value iteration
[[18.88 21.5  18.88 16.5  14.49]
 [16.58 18.88 16.58 14.56 12.78]
 [14.56 16.58 14.56 12.78 11.22]
 [12.78 14.56 12.78 11.22  9.85]
 [11.22 12.78 11.22  9.85  8.65]]
optimal policy after value iteration
  e    nesw   w    nesw   w   

  e     n     nw    w     w   

  e     n     nw    nw    nw  

  e     n     nw    nw    nw  

  e     n     nw    nw    nw  

optimal value function after value iteration
[[10. 10. 10.  5. 10.]
 [10. 10. 10. 10. 10.]
 [10. 10. 10.  0. 10.]
 [10. 10. 10. 10. 10.]
 [10.  0. 10. 10. 10.]]
optimal value function after value iteration
[[ 8.78 10.    8.78  5.    5.22]
 [ 7.71  8.78  7.71  6.77  5.94]
 [ 6.77  7.71  6.77  0.    5.22]
 [ 5.94  6.77  5.94  5.22  4.58]
 [ 5.22  0.    5.22  4.58  4.02]]
optimal value function after value iteration
[[ 7.62 10.    7.62  5.    3.81]
 [ 5.8   7.62  5.8   4.42  3.37]
 [ 4.42  5.8   4.42  0.    2.57]
 [ 3.37  4.42  3.37  2.57  1.96]
 [ 2.57  0.    2.57  1.96  1.49]]
